{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frank Schiavone\n",
    "EECE5644 Final Project\n",
    "Neural Networks\n",
    "\n",
    "Reference\n",
    "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class5_class_reg.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure, show\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Douglas\n",
    "def reduce_columns(df):\n",
    "    with open('../column_mapping.json', 'r') as f:\n",
    "        column_mapping = json.load(f)\n",
    "    # Important columns: YearsProgram, YearsCodedJob, Country, ImportantBenifits, CompanyType\n",
    "    original_columns_to_keep = ['YearsProgram', 'YearsCodedJob', 'Country', 'ImportantBenefits', 'CompanyType', 'Salary']\n",
    "    # def keep_columns(df, original_columns_to_keep):\n",
    "    cleaned_columns_to_keep = []\n",
    "    for original_col in original_columns_to_keep:\n",
    "        cleaned_columns_to_keep += column_mapping[original_col]\n",
    "    return df[cleaned_columns_to_keep].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shuffled Dataset\n",
    "filename_read = r\"c:\\Users\\fr23505\\Documents\\machine\\new_repo\\EECE5644-Project-master\\shuffled.csv\"\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'], index_col='Respondent')\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Create a dataset with reduced Parameters\n",
    "dfr = reduce_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model_layer_1(x_train, y_train, x_test, y_test, activ_func, learn_algo):\n",
    "    # Create the Neural Network Model\n",
    "    # Squential Network with up to 3 layers.\n",
    "    # Layer 1: nFeatures nodes using the rectifier\n",
    "    # Layer n: 1 node (regression)\n",
    "    # NN using optimization adam\n",
    "\n",
    "    # https://keras.io\n",
    "    # https://keras.io/layers/core/\n",
    "    # https://keras.io/losses/\n",
    "    # https://keras.io/optimizers/\n",
    "\n",
    "    # Optimization\n",
    "    # Error function: mean squared error\n",
    "    # Optimizers:\n",
    "    # SGD - stochastic gradient descent\n",
    "    # RMSprop\n",
    "    # Adagrad - Adaptive gradient\n",
    "    # Adadelta\n",
    "    # Adam - Adaptive Moment Estimation\n",
    "    # Adamax\n",
    "    # Nadam - Nesterov Adam optimizer\n",
    "\n",
    "    # Activation Functions\n",
    "    # softmax\n",
    "    # elu - Exponential Linear Unit\n",
    "    # selu - Scaled Exponential Linear Unit\n",
    "    # softplus\n",
    "    # softsign\n",
    "    # relu - rectified linear unit\n",
    "    # tanh - hyperbolic tangent\n",
    "    # sigmoid\n",
    "    # hard_sigmoid\n",
    "    # linear\n",
    "\n",
    "    num_neurons_1 = x_train.shape[1];\n",
    "    num_neurons_2 = round(x_train.shape[1]/2);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons_1, input_dim=x_train.shape[1], activation=activ_func))\n",
    "    model.add(Dense(num_neurons_2, activation=activ_func))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=learn_algo)\n",
    "    \n",
    "    # Learning Algorithm monitor: Stop training when\n",
    "    # validation loss is less the 1e-3 5 times in a row\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto')\n",
    "    \n",
    "    # Save weights for NN\n",
    "    checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=0, save_best_only=True)\n",
    "    \n",
    "    # Train NN\n",
    "    # The model will not be trained on this data.\n",
    "    # epochs=1000, Epoch: # complete iterations of the data set to be learned\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=0,epochs=1000)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model_layer_2(x_train, y_train, x_test, y_test, activ_func, learn_algo):\n",
    "    # Create the Neural Network Model\n",
    "    # Squential Network with up to 3 layers.\n",
    "    # Layer 1: nFeatures nodes using the rectifier\n",
    "    # Layer n: 1 node (regression)\n",
    "    # NN using optimization adam\n",
    "\n",
    "    # https://keras.io\n",
    "    # https://keras.io/layers/core/\n",
    "    # https://keras.io/losses/\n",
    "    # https://keras.io/optimizers/\n",
    "\n",
    "    # Optimization\n",
    "    # Error function: mean squared error\n",
    "    # Optimizers:\n",
    "    # SGD - stochastic gradient descent\n",
    "    # RMSprop\n",
    "    # Adagrad - Adaptive gradient\n",
    "    # Adadelta\n",
    "    # Adam - Adaptive Moment Estimation\n",
    "    # Adamax\n",
    "    # Nadam - Nesterov Adam optimizer\n",
    "\n",
    "    # Activation Functions\n",
    "    # softmax\n",
    "    # elu - Exponential Linear Unit\n",
    "    # selu - Scaled Exponential Linear Unit\n",
    "    # softplus\n",
    "    # softsign\n",
    "    # relu - rectified linear unit\n",
    "    # tanh - hyperbolic tangent\n",
    "    # sigmoid\n",
    "    # hard_sigmoid\n",
    "    # linear\n",
    "\n",
    "    num_neurons_1 = x_train.shape[1];\n",
    "    num_neurons_2 = round(x_train.shape[1]/2);\n",
    "    num_neurons_3 = round(x_train.shape[1]/4);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons_1, input_dim=x_train.shape[1], activation=activ_func))\n",
    "    model.add(Dense(num_neurons_2, activation=activ_func))\n",
    "    model.add(Dense(num_neurons_3, activation=activ_func))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=learn_algo)\n",
    "    \n",
    "    # Learning Algorithm monitor: Stop training when\n",
    "    # validation loss is less the 1e-3 5 times in a row\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto')\n",
    "    \n",
    "    # Save weights for NN\n",
    "    checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=0, save_best_only=True)\n",
    "    \n",
    "    # Train NN\n",
    "    # epochs=1000, Epoch: # complete iterations of the data set to be learned\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=0,epochs=1000)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model_layer_3(x_train, y_train, x_test, y_test, activ_func, learn_algo):\n",
    "    # Create the Neural Network Model\n",
    "    # Squential Network with up to 3 layers.\n",
    "    # Layer 1: nFeatures nodes using the rectifier\n",
    "    # Layer n: 1 node (regression)\n",
    "    # NN using optimization adam\n",
    "\n",
    "    # https://keras.io\n",
    "    # https://keras.io/layers/core/\n",
    "    # https://keras.io/losses/\n",
    "    # https://keras.io/optimizers/\n",
    "\n",
    "    # Optimization\n",
    "    # Error function: mean squared error\n",
    "    # Optimizers:\n",
    "    # SGD - stochastic gradient descent\n",
    "    # RMSprop\n",
    "    # Adagrad - Adaptive gradient\n",
    "    # Adadelta\n",
    "    # Adam - Adaptive Moment Estimation\n",
    "    # Adamax\n",
    "    # Nadam - Nesterov Adam optimizer\n",
    "\n",
    "    # Activation Functions\n",
    "    # softmax\n",
    "    # elu - Exponential Linear Unit\n",
    "    # selu - Scaled Exponential Linear Unit\n",
    "    # softplus\n",
    "    # softsign\n",
    "    # relu - rectified linear unit\n",
    "    # tanh - hyperbolic tangent\n",
    "    # sigmoid\n",
    "    # hard_sigmoid\n",
    "    # linear\n",
    "\n",
    "    num_neurons_1 = x_train.shape[1];\n",
    "    num_neurons_2 = round(x_train.shape[1]/2);\n",
    "    num_neurons_3 = round(x_train.shape[1]/4);\n",
    "    num_neurons_4 = round(x_train.shape[1]/8);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons_1, input_dim=x_train.shape[1], activation=activ_func))\n",
    "    model.add(Dense(num_neurons_2, activation=activ_func))\n",
    "    model.add(Dense(num_neurons_3, activation=activ_func))\n",
    "    model.add(Dense(num_neurons_4, activation=activ_func))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=learn_algo)\n",
    "    \n",
    "    # Learning Algorithm monitor: Stop training when\n",
    "    # validation loss is less the 1e-3 5 times in a row\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto')\n",
    "    \n",
    "    # Save weights for NN\n",
    "    checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=0, save_best_only=True)\n",
    "    \n",
    "    # Train NN\n",
    "    # epochs=1000, Epoch: # complete iterations of the data set to be learned\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=0,epochs=1000)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(pred, y_test): \n",
    "    errors = []\n",
    "    for index in range(len(pred)):\n",
    "        try:\n",
    "            errors.append(pred[index][0] - y_test[index])\n",
    "        except KeyError:\n",
    "            pass\n",
    "             # If we get a key error then we can't compare the prediction and error so we should just continue\n",
    "    plt.hist(errors, bins=30, histtype='step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_prediction(model, x_test, y_test):    \n",
    "    # load weights from Training NN\n",
    "    model.load_weights('best_weights.hdf5') \n",
    "\n",
    "    # Run Prediction\n",
    "    pred = model.predict(x_test)\n",
    "\n",
    "     # Calculate RMS\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    \n",
    "    #hist_plot(pred, y_test)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_fold_rmsd(df, activ_func, learn_algo):\n",
    "    rmse_sum_1 = 0\n",
    "    rmse_sum_2 = 0\n",
    "    rmse_sum_3 = 0\n",
    "    \n",
    "    fold=10;\n",
    "    for i in range(fold):\n",
    "        print(\"Iterator {}\".format(i))\n",
    "        # 1/10 of Dataset\n",
    "        test = df.iloc[int(len(df) * i/fold): int(len(df) * (i+1)/fold)]\n",
    "        \n",
    "        # Seperate Data into train and test\n",
    "        train = df.drop(test.index)\n",
    "\n",
    "        # Seperate targets\n",
    "        x_train = train.drop(columns='Salary')\n",
    "        y_train = train['Salary']\n",
    "        x_test = test.drop(columns='Salary')\n",
    "        y_test = test['Salary']\n",
    "        \n",
    "        # Create NN Models with 1 layer\n",
    "        rms = 0;\n",
    "        model_1 = nn_model_layer_1(x_train, y_train, x_test, y_test, activ_func, learn_algo)\n",
    "        \n",
    "        # Prediction\n",
    "        rms = nn_prediction(model_1, x_test, y_test)\n",
    "        print(\"Layer 1 RMS: {}\".format(rms))\n",
    "\n",
    "        # Delete Model\n",
    "        os.remove(\"best_weights.hdf5\")\n",
    "        del model_1\n",
    "\n",
    "        rmse_sum_1 += rms\n",
    "        \n",
    "        # Create NN Models with 2 layer\n",
    "        rms = 0;\n",
    "        model_2 = nn_model_layer_2(x_train, y_train, x_test, y_test, activ_func, learn_algo)\n",
    "        \n",
    "        # Prediction\n",
    "        rms = nn_prediction(model_2, x_test, y_test)\n",
    "        print(\"Layer 2 RMS: {}\".format(rms))\n",
    "\n",
    "        # Delete Model\n",
    "        os.remove(\"best_weights.hdf5\")\n",
    "        del model_2\n",
    "\n",
    "        rmse_sum_2 += rms\n",
    "        \n",
    "        # Create NN Models with 3 layer\n",
    "        rms = 0;\n",
    "        model_3 = nn_model_layer_3(x_train, y_train, x_test, y_test, activ_func, learn_algo)\n",
    "        \n",
    "        # Prediction\n",
    "        rms = nn_prediction(model_3, x_test, y_test)\n",
    "        print(\"Layer 3 RMS: {}\".format(rms))\n",
    "\n",
    "        # Delete Model\n",
    "        os.remove(\"best_weights.hdf5\")\n",
    "        del model_3\n",
    "\n",
    "        rmse_sum_3 += rms\n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"\\nAverage Layer 1 RMS: {}\".format(rmse_sum_1/10))\n",
    "    print(\"\\nAverage Layer 2 RMS: {}\".format(rmse_sum_2/10))\n",
    "    print(\"\\nAverage Layer 3 RMS: {}\".format(rmse_sum_3/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning Algorithm: Adam\")\n",
    "print(\"Activiation Function: Linear\")\n",
    "ten_fold_rmsd(df, 'linear', 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning Algorithm: Adam\")\n",
    "print(\"Activiation Function: Rectified Linear Unit\")\n",
    "ten_fold_rmsd(df, 'relu', 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning Algorithm: Adam\")\n",
    "print(\"Activiation Function: Softplus\")\n",
    "ten_fold_rmsd(df, 'softplus', 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning Algorithm: RMSprop\")\n",
    "print(\"Activiation Function: Linear\")\n",
    "ten_fold_rmsd(df, 'linear', 'RMSprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Learning Algorithm: RMSprop\")\n",
    "print(\"Activiation Function: Rectified Linear Unit\")\n",
    "ten_fold_rmsd(df, 'relu', 'RMSprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Learning Algorithm: RMSprop\")\n",
    "print(\"Activiation Function: Softplus\")\n",
    "ten_fold_rmsd(df, 'softplus', 'RMSprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activiation Function: Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Learning Algorithm: Adam\")\n",
    "print(\"Activiation Function: Sigmoid\")\n",
    "ten_fold_rmsd(df, 'sigmoid', 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning Algorithm: RMSprop\")\n",
    "print(\"Activiation Function: Sigmoid\")\n",
    "ten_fold_rmsd(df, 'sigmoid', 'RMSprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Dimensional Reduced dataset Test\n",
    "print(\"Learning Algorithm: Adam\")\n",
    "print(\"Activiation Function: Linear\")\n",
    "ten_fold_rmsd(dfr, 'linear', 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram Plot\n",
    "print(\"Learning Algorithm: Adam\")\n",
    "print(\"Activiation Function: Linear\")\n",
    "ten_fold_rmsd(df, 'linear', 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class5_class_reg.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
